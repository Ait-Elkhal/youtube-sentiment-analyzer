import pandas as pd
import os
import requests
from pathlib import Path

def download_reddit_dataset():
    """
    TÃ©lÃ©charge le dataset Reddit depuis GitHub
    """
    # URL du dataset
    url = "https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv"
    
    # Chemins des dossiers
    raw_data_path = Path("data/raw")
    raw_data_path.mkdir(parents=True, exist_ok=True)
    
    # Chemin complet du fichier
    file_path = raw_data_path / "reddit.csv"
    
    print("ğŸ“¥ TÃ©lÃ©chargement du dataset Reddit...")
    
    try:
        # TÃ©lÃ©charger le fichier
        response = requests.get(url)
        response.raise_for_status()  # VÃ©rifier les erreurs HTTP
        
        # Sauvegarder le fichier
        with open(file_path, 'wb') as f:
            f.write(response.content)
        
        print(f"âœ… Dataset tÃ©lÃ©chargÃ© avec succÃ¨s: {file_path}")
        
        # Charger et analyser le dataset
        df = pd.read_csv(file_path)
        
        # Renommer les colonnes pour la cohÃ©rence
        df = df.rename(columns={'clean_comment': 'text', 'category': 'label'})
        
        # Supprimer les lignes avec du texte manquant
        initial_count = len(df)
        df = df.dropna(subset=['text'])
        final_count = len(df)
        removed_count = initial_count - final_count
        
        print(f"ğŸ“Š Statistiques du dataset:")
        print(f"   - Commentaires initiaux: {initial_count}")
        print(f"   - Commentaires aprÃ¨s nettoyage: {final_count}")
        print(f"   - Commentaires supprimÃ©s (NaN): {removed_count}")
        
        # Distribution des labels
        print(f"   - Distribution des labels:")
        label_distribution = df['label'].value_counts().sort_index()
        for label, count in label_distribution.items():
            sentiment = {1: 'Positif', 0: 'Neutre', -1: 'NÃ©gatif'}.get(label, label)
            print(f"     {sentiment} ({label}): {count} Ã©chantillons ({count/len(df)*100:.1f}%)")
        
        # VÃ©rifier la taille minimale
        min_samples = 300
        adequate_sizes = all(count >= min_samples for count in label_distribution)
        if adequate_sizes:
            print(f"âœ… Taille adÃ©quate (au moins {min_samples} par classe)")
        else:
            print(f"âš ï¸  Certaines classes ont moins de {min_samples} Ã©chantillons")
        
        return df
        
    except Exception as e:
        print(f"âŒ Erreur lors du tÃ©lÃ©chargement: {e}")
        return None

def validate_dataset(df):
    """
    Valide la structure et la qualitÃ© du dataset
    """
    print("\nğŸ” Validation du dataset...")
    
    # VÃ©rifier les colonnes requises
    required_columns = ['text', 'label']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        print(f"âŒ Colonnes manquantes: {missing_columns}")
        return False
    else:
        print("âœ… Toutes les colonnes requises sont prÃ©sentes")
    
    # VÃ©rifier les valeurs manquantes
    missing_values = df.isnull().sum()
    print("ğŸ“‹ Valeurs manquantes par colonne:")
    for col, count in missing_values.items():
        print(f"   - {col}: {count} ({count/len(df)*100:.1f}%)")
    
    # VÃ©rifier les types de donnÃ©es
    print("ğŸ“ Types de donnÃ©es:")
    print(f"   - text: {df['text'].dtype}")
    print(f"   - label: {df['label'].dtype}")
    
    # VÃ©rifier la longueur des textes
    text_lengths = df['text'].str.len()
    print("ğŸ“ Statistiques de longueur de texte:")
    print(f"   - Moyenne: {text_lengths.mean():.1f} caractÃ¨res")
    print(f"   - MÃ©diane: {text_lengths.median():.1f} caractÃ¨res")
    print(f"   - Min: {text_lengths.min()} caractÃ¨res")
    print(f"   - Max: {text_lengths.max()} caractÃ¨res")
    
    return True

if __name__ == "__main__":
    # TÃ©lÃ©charger le dataset
    df = download_reddit_dataset()
    
    if df is not None:
        # Valider le dataset
        validate_dataset(df)
        
        # Sauvegarder la version standardisÃ©e
        output_path = Path("data/raw/reddit_standardized.csv")
        df.to_csv(output_path, index=False)
        print(f"\nğŸ’¾ Dataset standardisÃ© sauvegardÃ©: {output_path}")
        
        # AperÃ§u des donnÃ©es
        print("\nğŸ‘€ AperÃ§u des premiÃ¨res lignes:")
        print(df.head())
        
        print("\nğŸ¯ Exemples par sentiment:")
        for label in [-1, 0, 1]:
            sentiment_name = {1: 'Positif', 0: 'Neutre', -1: 'NÃ©gatif'}[label]
            sample = df[df['label'] == label].iloc[0]['text']
            print(f"   {sentiment_name} ({label}): {sample[:100]}...")