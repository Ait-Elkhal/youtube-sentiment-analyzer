# src/models/train_model.py
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.utils.class_weight import compute_class_weight
import time
import warnings
warnings.filterwarnings('ignore')

def main():
    """
    PHASE 3: D√©veloppement et entra√Ænement du mod√®le de classification de sentiment
    Conforme aux exigences du TP
    """
    print("=" * 70)
    print("PHASE 3: D√âVELOPPEMENT ET ENTRA√éNEMENT DU MOD√àLE")
    print("Conforme aux exigences du TP - Analyse de Sentiment YouTube")
    print("=" * 70)
    
    # Cr√©ation des dossiers
    Path("models/trained").mkdir(parents=True, exist_ok=True)
    Path("models/experiments").mkdir(parents=True, exist_ok=True)
    
    # 1. CHARGEMENT DES DONN√âES
    print("\nüì• 1. CHARGEMENT DES DONN√âES")
    X_train, X_test, y_train, y_test = load_and_prepare_data()
    
    # 2. VECTORISATION TF-IDF OPTIMIS√âE
    print("\nüîß 2. VECTORISATION TF-IDF AVEC PARAM√àTRES OPTIMIS√âS")
    vectorizer, X_train_tfidf, X_test_tfidf = create_optimized_tfidf(X_train, X_test)
    
    # 3. ENTRA√éNEMENT LOGISTIC REGRESSION AVEC OPTIMISATION
    print("\nüß† 3. LOGISTIC REGRESSION - OPTIMISATION HYPERPARAM√àTRES")
    lr_model, lr_metrics = train_logistic_regression(X_train_tfidf, y_train, X_test_tfidf, y_test)
    
    # 4. EXP√âRIMENTATION AVEC D'AUTRES ALGORITHMES
    print("\nüî¨ 4. EXP√âRIMENTATION AVEC D'AUTRES ALGORITHMES")
    rf_model, rf_metrics = train_random_forest(X_train_tfidf, y_train, X_test_tfidf, y_test)
    svm_model, svm_metrics = train_svm(X_train_tfidf, y_train, X_test_tfidf, y_test)
    
    # 5. COMPARAISON ET S√âLECTION DU MEILLEUR MOD√àLE
    print("\nüèÜ 5. COMPARAISON ET S√âLECTION DU MEILLEUR MOD√àLE")
    best_model, best_model_name, best_metrics = select_best_model(
        lr_model, lr_metrics, rf_model, rf_metrics, svm_model, svm_metrics
    )
    
    # 6. √âVALUATION D√âTAILL√âE DU MEILLEUR MOD√àLE
    print("\nüìä 6. √âVALUATION D√âTAILL√âE AVEC M√âTRIQUES")
    evaluate_best_model(best_model, X_test_tfidf, y_test, best_model_name)
    
    # 7. TEST DES PERFORMANCES D'INF√âRENCE
    print("\n‚ö° 7. TEST DES PERFORMANCES D'INF√âRENCE")
    inference_metrics = test_inference_performance(best_model, vectorizer, X_test)
    
    # 8. SAUVEGARDE DES MOD√àLES ET RAPPORTS
    print("\nüíæ 8. SAUVEGARDE DES MOD√àLES ET G√âN√âRATION DE RAPPORTS")
    save_models_and_reports(best_model, vectorizer, best_metrics, inference_metrics, y_test, 
                          X_test_tfidf, best_model_name)
    
    # 9. V√âRIFICATION FINALE DES CRIT√àRES DU TP
    print("\n‚úÖ 9. V√âRIFICATION DES CRIT√àRES DE PERFORMANCE DU TP")
    verify_tp_criteria(best_metrics, inference_metrics)
    
    print("\n" + "=" * 70)
    print("üéâ PHASE 3 TERMIN√âE AVEC SUCC√àS!")
    print("=" * 70)

def load_and_prepare_data():
    """Charge et pr√©pare les donn√©es d'entra√Ænement et de test"""
    train_df = pd.read_csv("data/processed/train.csv")
    test_df = pd.read_csv("data/processed/test.csv")
    
    X_train = train_df['cleaned_text'].astype(str)
    y_train = train_df['label']
    X_test = test_df['cleaned_text'].astype(str)
    y_test = test_df['label']
    
    print(f"‚úÖ Donn√©es charg√©es:")
    print(f"   - Train: {len(X_train)} √©chantillons")
    print(f"   - Test: {len(X_test)} √©chantillons")
    
    # Distribution des classes
    print(f"üìä Distribution des classes (Train):")
    for label in [-1, 0, 1]:
        count = (y_train == label).sum()
        percentage = count / len(y_train) * 100
        sentiment = {1: 'Positif', 0: 'Neutre', -1: 'N√©gatif'}[label]
        print(f"   - {sentiment}: {count} √©chantillons ({percentage:.1f}%)")
    
    return X_train, X_test, y_train, y_test

def create_optimized_tfidf(X_train, X_test):
    """
    Impl√©mente un vectoriseur TF-IDF avec param√®tres optimis√©s
    Conforme √† l'exigence: 'Impl√©menter un vectoriseur TF-IDF avec param√®tres optimis√©s'
    """
    print("üîß Cr√©ation du vectoriseur TF-IDF optimis√©...")
    
    # Param√®tres optimis√©s bas√©s sur l'analyse du dataset
    vectorizer = TfidfVectorizer(
        max_features=5000,           # Limite la dimensionnalit√©
        ngram_range=(1, 2),          # Unigrams et bigrams
        stop_words='english',        # Supprime les stop words
        min_df=2,                    # Termes apparaissant au moins 2 fois
        max_df=0.95,                 # Termes apparaissant dans max 95% des documents
        sublinear_tf=True,           # Application log pour p√©naliser les termes fr√©quents
        norm='l2'                    # Normalisation L2 pour les vecteurs
    )
    
    start_time = time.time()
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)
    vectorization_time = time.time() - start_time
    
    print(f"‚úÖ Vectorisation TF-IDF termin√©e en {vectorization_time:.2f}s")
    print(f"   - Dimension des features: {X_train_tfidf.shape[1]}")
    print(f"   - Taille du vocabulaire: {len(vectorizer.vocabulary_)}")
    print(f"   - N-gram range: {vectorizer.ngram_range}")
    
    return vectorizer, X_train_tfidf, X_test_tfidf

def train_logistic_regression(X_train, y_train, X_test, y_test):
    """
    Entra√Æne une Logistic Regression avec optimisation des hyperparam√®tres
    Conforme √† l'exigence: 'Entra√Æner un mod√®le de Logistic Regression'
    """
    print("üß† Entra√Ænement de la Logistic Regression avec GridSearchCV...")
    
    # Calcul des poids des classes pour g√©rer le d√©s√©quilibre
    classes = np.unique(y_train)
    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
    class_weight_dict = dict(zip(classes, class_weights))
    
    # GridSearch pour l'optimisation des hyperparam√®tres
    param_grid = {
        'C': [0.1, 1, 10, 100],           # Force de r√©gularisation
        'penalty': ['l1', 'l2'],          # Type de r√©gularisation
        'solver': ['liblinear', 'saga'],  # Algorithmes d'optimisation
        'max_iter': [1000]                # Nombre maximum d'it√©rations
    }
    
    lr_model = LogisticRegression(
        class_weight=class_weight_dict,
        random_state=42
    )
    
    # GridSearchCV pour l'optimisation
    grid_search = GridSearchCV(
        lr_model, param_grid,
        cv=3,                    # 3-fold cross-validation
        scoring='f1_weighted',   # M√©trique d'optimisation
        n_jobs=-1,              # Utilisation de tous les cores
        verbose=1
    )
    
    print("üîç D√©but de l'optimisation des hyperparam√®tres...")
    start_time = time.time()
    grid_search.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    best_model = grid_search.best_estimator_
    
    # √âvaluation
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_per_class = f1_score(y_test, y_pred, average=None)
    
    print(f"‚úÖ Logistic Regression optimis√©e en {training_time:.2f}s")
    print(f"üéØ Meilleurs param√®tres: {grid_search.best_params_}")
    print(f"üìà Meilleur score CV: {grid_search.best_score_:.4f}")
    print(f"üß™ Performance test - Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
    
    metrics = {
        'model': best_model,
        'accuracy': accuracy,
        'f1_weighted': f1,
        'f1_per_class': f1_per_class,
        'best_params': grid_search.best_params_,
        'training_time': training_time
    }
    
    return best_model, metrics

def train_random_forest(X_train, y_train, X_test, y_test):
    """
    Exp√©rimente avec Random Forest
    Conforme √† l'exigence: 'Exp√©rimenter avec d'autres algorithmes (Random Forest, SVM, etc.)'
    """
    print("üå≤ Entra√Ænement de Random Forest avec RandomizedSearchCV...")
    
    # Calcul des poids des classes
    classes = np.unique(y_train)
    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
    class_weight_dict = dict(zip(classes, class_weights))
    
    # RandomizedSearch pour Random Forest (plus rapide que GridSearch)
    param_dist = {
        'n_estimators': [100, 200, 300],      # Nombre d'arbres
        'max_depth': [None, 10, 20, 30],      # Profondeur maximale
        'min_samples_split': [2, 5, 10],      # √âchantillons minimum pour diviser
        'min_samples_leaf': [1, 2, 4],        # √âchantillons minimum par feuille
        'max_features': ['sqrt', 'log2']      # Nombre de features pour split
    }
    
    rf_model = RandomForestClassifier(
        class_weight=class_weight_dict,
        random_state=42,
        n_jobs=-1
    )
    
    random_search = RandomizedSearchCV(
        rf_model, param_dist,
        n_iter=10,              # 10 combinaisons al√©atoires
        cv=3,                   # 3-fold cross-validation
        scoring='f1_weighted',  # M√©trique d'optimisation
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    
    start_time = time.time()
    random_search.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    best_model = random_search.best_estimator_
    
    # √âvaluation
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_per_class = f1_score(y_test, y_pred, average=None)
    
    print(f"‚úÖ Random Forest optimis√© en {training_time:.2f}s")
    print(f"üéØ Meilleurs param√®tres: {random_search.best_params_}")
    print(f"üìà Meilleur score CV: {random_search.best_score_:.4f}")
    print(f"üß™ Performance test - Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
    
    metrics = {
        'model': best_model,
        'accuracy': accuracy,
        'f1_weighted': f1,
        'f1_per_class': f1_per_class,
        'best_params': random_search.best_params_,
        'training_time': training_time
    }
    
    return best_model, metrics

def train_svm(X_train, y_train, X_test, y_test):
    """
    Exp√©rimente avec SVM
    Conforme √† l'exigence: 'Exp√©rimenter avec d'autres algorithmes (Random Forest, SVM, etc.)'
    """
    print("‚ö° Entra√Ænement de SVM (version optimis√©e)...")
    
    # SVM avec noyau lin√©aire pour efficacit√©
    svm_model = SVC(
        C=1.0,                    # Param√®tre de r√©gularisation
        kernel='linear',          # Noyau lin√©aire pour efficacit√©
        probability=True,         # Permet predict_proba
        random_state=42,
        class_weight='balanced'   # Gestion automatique du d√©s√©quilibre
    )
    
    start_time = time.time()
    svm_model.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    # √âvaluation
    y_pred = svm_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_per_class = f1_score(y_test, y_pred, average=None)
    
    print(f"‚úÖ SVM entra√Æn√© en {training_time:.2f}s")
    print(f"üß™ Performance test - Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
    
    metrics = {
        'model': svm_model,
        'accuracy': accuracy,
        'f1_weighted': f1,
        'f1_per_class': f1_per_class,
        'training_time': training_time
    }
    
    return svm_model, metrics

def select_best_model(lr_model, lr_metrics, rf_model, rf_metrics, svm_model, svm_metrics):
    """S√©lectionne le meilleur mod√®le bas√© sur le F1-score"""
    print("üèÜ Comparaison des mod√®les...")
    
    models_comparison = {
        'Logistic Regression': lr_metrics,
        'Random Forest': rf_metrics,
        'SVM': svm_metrics
    }
    
    # Affichage du tableau de comparaison
    print("\nüìä TABLEAU COMPARATIF DES MOD√àLES:")
    print("-" * 70)
    print(f"{'Mod√®le':<20} {'Accuracy':<10} {'F1-Score':<10} {'Temps (s)':<10}")
    print("-" * 70)
    
    best_f1 = 0
    best_model_name = ""
    best_model = None
    best_metrics = None
    
    for name, metrics in models_comparison.items():
        print(f"{name:<20} {metrics['accuracy']:.4f}    {metrics['f1_weighted']:.4f}    {metrics['training_time']:>8.1f}")
        
        if metrics['f1_weighted'] > best_f1:
            best_f1 = metrics['f1_weighted']
            best_model_name = name
            best_model = metrics['model']
            best_metrics = metrics
    
    print("-" * 70)
    print(f"üéØ MEILLEUR MOD√àLE: {best_model_name}")
    print(f"   - F1-Score: {best_metrics['f1_weighted']:.4f}")
    print(f"   - Accuracy: {best_metrics['accuracy']:.4f}")
    
    return best_model, best_model_name, best_metrics

def evaluate_best_model(model, X_test, y_test, model_name):
    """
    √âvalue le mod√®le avec m√©triques appropri√©es
    Conforme √† l'exigence: '√âvaluer avec m√©triques appropri√©es (accuracy, F1-score, matrice de confusion)'
    """
    print(f"üìä √âvaluation d√©taill√©e du mod√®le {model_name}...")
    
    # Pr√©dictions
    y_pred = model.predict(X_test)
    
    # M√©triques d√©taill√©es
    accuracy = accuracy_score(y_test, y_pred)
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    f1_per_class = f1_score(y_test, y_pred, average=None)
    
    print(f"üìà M√âTRIQUES D√âTAILL√âES:")
    print(f"   - Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   - F1-score (weighted): {f1_weighted:.4f}")
    print(f"   - F1-score par classe:")
    for i, label in enumerate([-1, 0, 1]):
        sentiment = {1: 'Positif', 0: 'Neutre', -1: 'N√©gatif'}[label]
        print(f"     {sentiment}: {f1_per_class[i]:.4f}")
    
    # Rapport de classification complet
    print(f"\nüìù RAPPORT DE CLASSIFICATION COMPLET:")
    print(classification_report(y_test, y_pred, 
                              target_names=['N√©gatif', 'Neutre', 'Positif'],
                              digits=4))
    
    # Matrice de confusion
    print("üìä G√©n√©ration de la matrice de confusion...")
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['N√©gatif', 'Neutre', 'Positif'],
                yticklabels=['N√©gatif', 'Neutre', 'Positif'])
    plt.title(f'Matrice de Confusion - {model_name}\nAccuracy: {accuracy:.4f}', fontsize=14)
    plt.xlabel('Pr√©diction', fontsize=12)
    plt.ylabel('V√©rit√© Terrain', fontsize=12)
    plt.tight_layout()
    plt.savefig('models/experiments/confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("‚úÖ Matrice de confusion sauvegard√©e")

def test_inference_performance(model, vectorizer, X_test):
    """
    Teste le temps d'inf√©rence pour v√©rifier le crit√®re de performance
    Conforme √† l'exigence: 'Temps d'inf√©rence < 100ms pour un batch de 50 commentaires'
    """
    print("‚ö° Test des performances d'inf√©rence...")
    
    # Test avec diff√©rents batch sizes
    batch_sizes = [1, 10, 50, 100]
    results = {}
    
    for batch_size in batch_sizes:
        # S√©lection al√©atoire de commentaires
        sample_indices = np.random.choice(len(X_test), batch_size, replace=False)
        sample_texts = X_test.iloc[sample_indices]
        
        # Mesure du temps d'inf√©rence
        start_time = time.time()
        
        # Vectorisation
        sample_tfidf = vectorizer.transform(sample_texts)
        vectorization_time = time.time() - start_time
        
        # Pr√©diction
        prediction_start = time.time()
        predictions = model.predict(sample_tfidf)
        prediction_time = time.time() - prediction_start
        
        total_time = time.time() - start_time
        
        results[batch_size] = {
            'total_time': total_time,
            'vectorization_time': vectorization_time,
            'prediction_time': prediction_time,
            'time_per_comment': total_time / batch_size,
            'comments_per_second': batch_size / total_time
        }
        
        print(f"   - Batch {batch_size:3d} comments: {total_time*1000:6.2f}ms "
              f"({total_time/batch_size*1000:5.2f}ms/comment)")
    
    # V√©rification sp√©cifique du crit√®re pour 50 commentaires
    inference_50 = results[50]
    criteria_met = inference_50['total_time'] < 0.1  # < 100ms
    
    print(f"\nüéØ CRIT√àRE D'INF√âRENCE - 50 commentaires:")
    print(f"   - Temps total: {inference_50['total_time']*1000:.2f}ms")
    print(f"   - Crit√®re: < 100ms")
    print(f"   - R√©sultat: {'‚úÖ ATTEINT' if criteria_met else '‚ùå NON ATTEINT'}")
    
    return {
        'inference_50_time': inference_50['total_time'],
        'criteria_met': criteria_met,
        'all_results': results
    }

def save_models_and_reports(model, vectorizer, metrics, inference_metrics, y_test, X_test, model_name):
    """
    Sauvegarde les mod√®les et g√©n√®re les rapports
    Conforme √† l'exigence: 'Sauvegarder le meilleur mod√®le et le vectoriseur avec joblib'
    """
    print("üíæ Sauvegarde des mod√®les et g√©n√©ration des rapports...")
    
    # Sauvegarde du mod√®le
    model_path = "models/trained/best_sentiment_model.joblib"
    joblib.dump(model, model_path)
    
    # Sauvegarde du vectoriseur
    vectorizer_path = "models/trained/tfidf_vectorizer.joblib"
    joblib.dump(vectorizer, vectorizer_path)
    
    # M√©triques compl√®tes
    full_metrics = {
        'model_name': model_name,
        'test_accuracy': metrics['accuracy'],
        'test_f1_weighted': metrics['f1_weighted'],
        'test_f1_per_class': metrics['f1_per_class'].tolist(),
        'inference_time_50': inference_metrics['inference_50_time'],
        'inference_criteria_met': inference_metrics['criteria_met'],
        'training_date': pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
        'dataset_info': {
            'train_size': len(X_test),  # Approximation
            'feature_dimension': X_test.shape[1]
        }
    }
    
    metrics_path = "models/trained/model_metrics.joblib"
    joblib.dump(full_metrics, metrics_path)
    
    # G√©n√©ration du rapport de performance
    generate_performance_report(full_metrics, model_name)
    
    print("‚úÖ Tous les fichiers sauvegard√©s:")
    print(f"   - Mod√®le: {model_path}")
    print(f"   - Vectoriseur: {vectorizer_path}")
    print(f"   - M√©triques: {metrics_path}")

def generate_performance_report(metrics, model_name):
    """G√©n√®re un rapport de performance d√©taill√©"""
    report = f"""
# üìä RAPPORT DE PERFORMANCE - MOD√àLE DE SENTIMENT

## üéØ INFORMATIONS G√âN√âRALES
- **Mod√®le**: {model_name}
- **Date d'entra√Ænement**: {metrics['training_date']}
- **Taille du dataset**: {metrics['dataset_info']['train_size']} √©chantillons
- **Dimension des features**: {metrics['dataset_info']['feature_dimension']}

## üìà PERFORMANCES

### M√©triques de Classification
- **Accuracy**: {metrics['test_accuracy']:.4f} ({metrics['test_accuracy']*100:.2f}%)
- **F1-Score (weighted)**: {metrics['test_f1_weighted']:.4f} ({metrics['test_f1_weighted']*100:.2f}%)

### F1-Score par Classe
- **N√©gatif**: {metrics['test_f1_per_class'][0]:.4f}
- **Neutre**: {metrics['test_f1_per_class'][1]:.4f}
- **Positif**: {metrics['test_f1_per_class'][2]:.4f}

### Performances d'Inf√©rence
- **Temps pour 50 commentaires**: {metrics['inference_time_50']*1000:.2f}ms
- **Crit√®re de performance**: {'‚úÖ ATTEINT' if metrics['inference_criteria_met'] else '‚ùå NON ATTEINT'}

## üîß D√âTAILS TECHNIQUES

### Vectorisation TF-IDF
- **max_features**: 5000
- **ngram_range**: (1, 2)
- **stop_words**: english
- **min_df**: 2
- **max_df**: 0.95

### Optimisation des Hyperparam√®tres
- **M√©thode**: GridSearchCV / RandomizedSearchCV
- **Scoring**: F1-score weighted
- **Cross-validation**: 3 folds

---
*Rapport g√©n√©r√© automatiquement - Phase 3 du TP Cloud Computing*
"""
    
    with open("models/experiments/performance_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("‚úÖ Rapport de performance g√©n√©r√©")

def verify_tp_criteria(metrics, inference_metrics):
    """
    V√©rifie que tous les crit√®res du TP sont atteints
    """
    print("‚úÖ V√âRIFICATION DES CRIT√àRES DU TP")
    print("-" * 50)
    
    # Crit√®re 1: Accuracy minimale 80%
    accuracy_ok = metrics['accuracy'] >= 0.80
    print(f"1. Accuracy ‚â• 80%: {metrics['accuracy']:.4f} {'‚úÖ' if accuracy_ok else '‚ùå'}")
    
    # Crit√®re 2: F1-score par classe > 0.75
    f1_ok = all(f1 >= 0.75 for f1 in metrics['f1_per_class'])
    f1_details = [f"{f1:.4f}" for f1 in metrics['f1_per_class']]
    print(f"2. F1-score par classe > 0.75: {f1_details} {'‚úÖ' if f1_ok else '‚ùå'}")
    
    # Crit√®re 3: Temps d'inf√©rence < 100ms pour 50 commentaires
    inference_ok = inference_metrics['inference_50_time'] < 0.1
    print(f"3. Temps inf√©rence < 100ms: {inference_metrics['inference_50_time']*1000:.2f}ms {'‚úÖ' if inference_ok else '‚ùå'}")
    
    # R√©sum√© final
    all_criteria_met = accuracy_ok and f1_ok and inference_ok
    print("-" * 50)
    print(f"üéØ TOUS LES CRIT√àRES: {'‚úÖ ATTEINTS' if all_criteria_met else '‚ùå NON ATTEINTS'}")
    
    if all_criteria_met:
        print("üéâ F√âLICITATIONS! La Phase 3 est compl√®tement conforme aux exigences du TP!")
    else:
        print("‚ö†Ô∏è  Certains crit√®res ne sont pas atteints. Am√©liorations n√©cessaires.")

if __name__ == "__main__":
    main()